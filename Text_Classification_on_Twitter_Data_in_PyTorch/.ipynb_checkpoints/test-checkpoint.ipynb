{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bwJIfh0IB7r"
   },
   "source": [
    "## Import dependencies\n",
    "```diff\n",
    "+ You can change below code.\n",
    "@@ In the report, write which additional packages you used.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ly6eY8PhIB7t"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import glob\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYO0G-jfIB7u"
   },
   "source": [
    "### Import meta info (tokens, number of users )\n",
    "```diff\n",
    "- You should not change below code.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ubck-S_sIB7v"
   },
   "outputs": [],
   "source": [
    "meta = json.load(open('./meta.json', 'r'))\n",
    "tokens = meta['tokens']\n",
    "num_token = len(tokens)\n",
    "num_user = meta['num_user']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLhjLNmKIB7v"
   },
   "source": [
    "### Load test dataset \n",
    "```diff\n",
    "- You should not change below code.\n",
    "@@ Real test dataset will not be distributed. \n",
    "@@ For now, Make this notebook work with `valid.json`.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "we739KwHIB7w"
   },
   "outputs": [],
   "source": [
    "test_data = json.load(open('./valid.json', 'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miB0GGhDIB7x"
   },
   "source": [
    "### Define Dataset and DataLoader\n",
    "```diff\n",
    "- You should not change below code.\n",
    "- Make your model get the input of sample['token_id']\n",
    "@@ Even if you batchfy the data when training the model, below code can/should work.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FM_tQEBHIB7x"
   },
   "outputs": [],
   "source": [
    "class tweetDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        sample['token_id'] = torch.Tensor(sample['token_id'])\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "QLgDjQh6IB7y"
   },
   "outputs": [],
   "source": [
    "test_dataset = tweetDataset(test_data)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "sample = next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GdvbjmQvIB7y"
   },
   "source": [
    "## DEFINE YOUR MODEL IN THE BELOW CELL.\n",
    "```diff\n",
    "+ You can change below code.\n",
    "- Name the class as \"Model\", DO NOT CHANGE THE NAME OF THE BELOW CLASS! \n",
    "@@ In the report, write which model/method you tried, and what is the final model that will be submitted.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4B_RhOaYIB7z"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_token, num_user, embed_dim, rnn_dim, num_layers):\n",
    "        super(Model, self).__init__()\n",
    "        self.num_token = num_token\n",
    "        self.num_user = num_user\n",
    "        self.embed_dim = embed_dim\n",
    "        self.rnn_dim = rnn_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_token + 1, embed_dim)\n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.out_linear = nn.Linear(rnn_dim, num_user)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, token_id):\n",
    "        embed = self.embedding(token_id)\n",
    "        embed = self.dropout(embed)\n",
    "        out, _ = self.rnn(embed)\n",
    "        return self.out_linear(out[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qa7TR5UoIB7z"
   },
   "source": [
    "### Load the best model\n",
    "```diff\n",
    "- Name your best model as [FirstName]_[LastName].pth\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DIWJfydlIB70",
    "outputId": "909de388-56a0-434c-959c-b5b974f617a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish loaded model.\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "# Name your best model as [FirstName]_[LastName].pth\n",
    "best_path = './abdullah_oezbay.pth'\n",
    "\n",
    "model = Model(num_token, num_user, embed_dim=512, rnn_dim=1024, num_layers=1).to(device)\n",
    "model.load_state_dict(torch.load(best_path))\n",
    "model.eval()\n",
    "\n",
    "print(\"Finish loaded model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wU7OVnbIB71"
   },
   "source": [
    "### Number of parameter information\n",
    "```diff\n",
    "- The number of parameters should not exceed 20,000,000 !!\n",
    "- DO NOT USE TRANSFORMER-BASED MODELS!!\n",
    "@@ Transformer-based models will not be accepted as a submission.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QVP8wYMaIB72",
    "outputId": "cbc14101-bac5-4da2-f5c4-f581486d3c18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 13153288\n",
      "[NOTE] Number of parameters SHOULD NOT exceed 20,000,000 (20 million).\n"
     ]
    }
   ],
   "source": [
    "num_param = sum(p.numel() for p in model.parameters())\n",
    "print('Number of parameters: {}'.format(num_param))\n",
    "print('[NOTE] Number of parameters SHOULD NOT exceed 20,000,000 (20 million).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuu9o_kwIB72"
   },
   "source": [
    "### Test the model\n",
    "```diff\n",
    "- BELOW CODE CELL SHOULD WORK WITH YOUR MODEL!\n",
    "@@ Check if your model returns the prediction with shape: BATCH by NUM_USER (NUM_OUTPUT).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k14_LrdpIB73",
    "outputId": "9601ad0c-aa80-4d89-8a17-b1b3a90bd9dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape would be BATCH X NUM_OUTPUT :  torch.Size([1, 8])\n"
     ]
    }
   ],
   "source": [
    "# Test the model if it generates proper output which shape is B by num_user\n",
    "pred = model(sample['token_id'].long().to(device))\n",
    "print('Prediction shape would be BATCH X NUM_OUTPUT : ', pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OA0P7w3yIB73"
   },
   "source": [
    "### Run test\n",
    "```diff\n",
    "- Do not change below code!\n",
    "- Your model should work with below code without any modification.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YUbVx-rcIB74",
    "outputId": "a22d166f-cfb9-4de7-b450-c74c780c6b5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOUR TEST ACCURACY: 0.7752808928489685\n"
     ]
    }
   ],
   "source": [
    "correct_cnt = 0.0\n",
    "data_cnt = 0.0\n",
    "for sample in test_dataloader:\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(sample['token_id'].long().to(device))\n",
    "\n",
    "    pred_user_id = torch.argmax(pred, dim=-1)\n",
    "\n",
    "    accu = pred_user_id.detach().cpu() == sample['user_id']\n",
    "\n",
    "    correct_cnt += torch.sum(accu)\n",
    "    data_cnt += sample['token_id'].shape[0]\n",
    "\n",
    "# calculate best valid accuracy\n",
    "test_accu = (correct_cnt / data_cnt).item()\n",
    "print('YOUR TEST ACCURACY: {}'.format(test_accu))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
